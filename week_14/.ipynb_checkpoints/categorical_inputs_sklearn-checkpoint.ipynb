{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with categorical inputs in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the example data set from Week 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/jyurko/CMPINF_2100_Fall_2020/master/week_11/week_11_example_data.csv'\n",
    "df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x1      300 non-null    float64\n",
      " 1   x2      300 non-null    float64\n",
      " 2   x3      300 non-null    float64\n",
      " 3   x4      300 non-null    float64\n",
      " 4   x5      300 non-null    object \n",
      " 5   y       300 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 14.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(data = df, kind='box', aspect=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model with a discrete input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stasmodels as a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.588229\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "stats_fit = smf.logit( formula = 'y ~ x1 + x2 + x3 + x4 + x5', data = df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  300\n",
      "Model:                          Logit   Df Residuals:                      293\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 20 Nov 2020   Pseudo R-squ.:                 0.08238\n",
      "Time:                        01:22:25   Log-Likelihood:                -176.47\n",
      "converged:                       True   LL-Null:                       -192.31\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.876e-05\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.6745      0.254     -2.653      0.008      -1.173      -0.176\n",
      "x5[T.b]       -0.2898      0.314     -0.922      0.357      -0.906       0.326\n",
      "x5[T.c]        0.3100      0.354      0.876      0.381      -0.383       1.003\n",
      "x1             0.3104      0.130      2.383      0.017       0.055       0.566\n",
      "x2            -0.5796      0.137     -4.220      0.000      -0.849      -0.310\n",
      "x3             0.1139      0.130      0.877      0.380      -0.140       0.368\n",
      "x4            -0.0488      0.130     -0.374      0.709      -0.304       0.207\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print( stats_fit.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the design matrix to see the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydmat, Xdmat = dmatrices( 'y ~ x1 + x2 + x3 + x4 + x5', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x5[T.b]</th>\n",
       "      <th>x5[T.c]</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.077412</td>\n",
       "      <td>-0.991094</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>-1.019744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.637747</td>\n",
       "      <td>-0.307506</td>\n",
       "      <td>-1.794250</td>\n",
       "      <td>1.189993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.345143</td>\n",
       "      <td>-1.075281</td>\n",
       "      <td>1.685044</td>\n",
       "      <td>0.167879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.659924</td>\n",
       "      <td>0.735156</td>\n",
       "      <td>-0.210541</td>\n",
       "      <td>-2.534105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.182378</td>\n",
       "      <td>1.599140</td>\n",
       "      <td>-0.301995</td>\n",
       "      <td>1.206648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.239739</td>\n",
       "      <td>1.572756</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>1.402896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.528292</td>\n",
       "      <td>3.120873</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>-0.599724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256334</td>\n",
       "      <td>1.293508</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.996928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.628451</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>2.197234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250264</td>\n",
       "      <td>1.105026</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>-0.584460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Intercept  x5[T.b]  x5[T.c]        x1        x2        x3        x4\n",
       "0          1.0      0.0      1.0  1.077412 -0.991094  0.208610 -1.019744\n",
       "1          1.0      1.0      0.0 -0.637747 -0.307506 -1.794250  1.189993\n",
       "2          1.0      1.0      0.0  1.345143 -1.075281  1.685044  0.167879\n",
       "3          1.0      1.0      0.0  1.659924  0.735156 -0.210541 -2.534105\n",
       "4          1.0      1.0      0.0  1.182378  1.599140 -0.301995  1.206648\n",
       "..         ...      ...      ...       ...       ...       ...       ...\n",
       "295        1.0      0.0      1.0  1.239739  1.572756  0.610548  1.402896\n",
       "296        1.0      0.0      1.0 -0.528292  3.120873  0.357688 -0.599724\n",
       "297        1.0      1.0      0.0  0.256334  1.293508  0.023925 -0.996928\n",
       "298        1.0      1.0      0.0 -0.628451  0.373513  0.038140  2.197234\n",
       "299        1.0      1.0      0.0  1.250264  1.105026  0.593161 -0.584460\n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame( Xdmat, columns=stats_fit.params.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to fit in sklearn, we would want to **NOT** use the intercept column of ones. So instead of using dummy variables we would **one-hot encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysk, Xsk = dmatrices( 'y ~ x1 + x2 + x3 + x4 + x5 - 1', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.077412</td>\n",
       "      <td>-0.991094</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>-1.019744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.637747</td>\n",
       "      <td>-0.307506</td>\n",
       "      <td>-1.794250</td>\n",
       "      <td>1.189993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.345143</td>\n",
       "      <td>-1.075281</td>\n",
       "      <td>1.685044</td>\n",
       "      <td>0.167879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.659924</td>\n",
       "      <td>0.735156</td>\n",
       "      <td>-0.210541</td>\n",
       "      <td>-2.534105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.182378</td>\n",
       "      <td>1.599140</td>\n",
       "      <td>-0.301995</td>\n",
       "      <td>1.206648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.239739</td>\n",
       "      <td>1.572756</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>1.402896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.528292</td>\n",
       "      <td>3.120873</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>-0.599724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256334</td>\n",
       "      <td>1.293508</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.996928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.628451</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>2.197234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250264</td>\n",
       "      <td>1.105026</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>-0.584460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2         3         4         5         6\n",
       "0    0.0  0.0  1.0  1.077412 -0.991094  0.208610 -1.019744\n",
       "1    0.0  1.0  0.0 -0.637747 -0.307506 -1.794250  1.189993\n",
       "2    0.0  1.0  0.0  1.345143 -1.075281  1.685044  0.167879\n",
       "3    0.0  1.0  0.0  1.659924  0.735156 -0.210541 -2.534105\n",
       "4    0.0  1.0  0.0  1.182378  1.599140 -0.301995  1.206648\n",
       "..   ...  ...  ...       ...       ...       ...       ...\n",
       "295  0.0  0.0  1.0  1.239739  1.572756  0.610548  1.402896\n",
       "296  0.0  0.0  1.0 -0.528292  3.120873  0.357688 -0.599724\n",
       "297  0.0  1.0  0.0  0.256334  1.293508  0.023925 -0.996928\n",
       "298  0.0  1.0  0.0 -0.628451  0.373513  0.038140  2.197234\n",
       "299  0.0  1.0  0.0  1.250264  1.105026  0.593161 -0.584460\n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame( Xsk )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression model with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit with the one-hot encoding in sklearn, we should set `fit_intercept` to False.  \n",
    "\n",
    "**Important**: if we were fitting a model with only continuous variables, and we created the design matrix with `dmatrices()` and we did not include the intercept column of ones (by including the the `-1` term in the formula), we would set `fit_intercept=True` in the sklearn call.  \n",
    "\n",
    "In this application, we will set `fit_intercept=False` because of the one-hot encoding.\n",
    "\n",
    "Also, note that the `penalty` argument is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_fit_mle = LogisticRegression(penalty='none', solver='lbfgs', fit_intercept=False).fit(Xsk, ysk.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, let's confirm that the intercpept was **not** fit and is equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_fit_mle.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now directly compare the statsmodels coefficient estimates with the sklearn estimates. Note that the DataFrame index is labeled as `Intercept` for the zeroth row. That is true for the statsmodels coefficient, but the sklearn coefficient is actually the one-hot encoding column associated with `x5='a'`. Notice that they have the same coefficient value! Because they have the same interpretation! The coefficients associated with `x5='b'` and `x5='c'` however are different. That is due to dummy variables are the **relative** effect compared to the reference level stored in the intercept. While the one-hot encoding formulation is estimate associated with that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statsmodels</th>\n",
       "      <th>sklearn_no_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-0.674470</td>\n",
       "      <td>-0.674465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5[T.b]</th>\n",
       "      <td>-0.289844</td>\n",
       "      <td>-0.964304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5[T.c]</th>\n",
       "      <td>0.310042</td>\n",
       "      <td>-0.364434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.310394</td>\n",
       "      <td>0.310406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.579591</td>\n",
       "      <td>-0.579577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.113856</td>\n",
       "      <td>0.113846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>-0.048754</td>\n",
       "      <td>-0.048760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           statsmodels  sklearn_no_penalty\n",
       "Intercept    -0.674470           -0.674465\n",
       "x5[T.b]      -0.289844           -0.964304\n",
       "x5[T.c]       0.310042           -0.364434\n",
       "x1            0.310394            0.310406\n",
       "x2           -0.579591           -0.579577\n",
       "x3            0.113856            0.113846\n",
       "x4           -0.048754           -0.048760"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'statsmodels': stats_fit.params,\n",
    "             'sklearn_no_penalty': list(sk_fit_mle.coef_.ravel())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dummy variables with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pd.get_dummies()` function to create the dummy variables or the one-hot encoding from the Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract the input and output features\n",
    "xinput = df.loc[:, ['x1', 'x2', 'x3', 'x4', 'x5']].copy()\n",
    "\n",
    "youtput = df.loc[:, ['y']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the one-hot encoding with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfeatures_onehot = pd.get_dummies(xinput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, examine the input feature DataFrame. Look at the `x5` column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.077412</td>\n",
       "      <td>-0.991094</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>-1.019744</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.637747</td>\n",
       "      <td>-0.307506</td>\n",
       "      <td>-1.794250</td>\n",
       "      <td>1.189993</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.345143</td>\n",
       "      <td>-1.075281</td>\n",
       "      <td>1.685044</td>\n",
       "      <td>0.167879</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.659924</td>\n",
       "      <td>0.735156</td>\n",
       "      <td>-0.210541</td>\n",
       "      <td>-2.534105</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.182378</td>\n",
       "      <td>1.599140</td>\n",
       "      <td>-0.301995</td>\n",
       "      <td>1.206648</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.239739</td>\n",
       "      <td>1.572756</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>1.402896</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.528292</td>\n",
       "      <td>3.120873</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>-0.599724</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.256334</td>\n",
       "      <td>1.293508</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.996928</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.628451</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>2.197234</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.250264</td>\n",
       "      <td>1.105026</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>-0.584460</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4 x5\n",
       "0    1.077412 -0.991094  0.208610 -1.019744  c\n",
       "1   -0.637747 -0.307506 -1.794250  1.189993  b\n",
       "2    1.345143 -1.075281  1.685044  0.167879  b\n",
       "3    1.659924  0.735156 -0.210541 -2.534105  b\n",
       "4    1.182378  1.599140 -0.301995  1.206648  b\n",
       "..        ...       ...       ...       ... ..\n",
       "295  1.239739  1.572756  0.610548  1.402896  c\n",
       "296 -0.528292  3.120873  0.357688 -0.599724  c\n",
       "297  0.256334  1.293508  0.023925 -0.996928  b\n",
       "298 -0.628451  0.373513  0.038140  2.197234  b\n",
       "299  1.250264  1.105026  0.593161 -0.584460  b\n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at the additional *augmented* features associated with the one-hot encoding of the `x5` categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5_a</th>\n",
       "      <th>x5_b</th>\n",
       "      <th>x5_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.077412</td>\n",
       "      <td>-0.991094</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>-1.019744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.637747</td>\n",
       "      <td>-0.307506</td>\n",
       "      <td>-1.794250</td>\n",
       "      <td>1.189993</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.345143</td>\n",
       "      <td>-1.075281</td>\n",
       "      <td>1.685044</td>\n",
       "      <td>0.167879</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.659924</td>\n",
       "      <td>0.735156</td>\n",
       "      <td>-0.210541</td>\n",
       "      <td>-2.534105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.182378</td>\n",
       "      <td>1.599140</td>\n",
       "      <td>-0.301995</td>\n",
       "      <td>1.206648</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.239739</td>\n",
       "      <td>1.572756</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>1.402896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.528292</td>\n",
       "      <td>3.120873</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>-0.599724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.256334</td>\n",
       "      <td>1.293508</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.996928</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.628451</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>2.197234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.250264</td>\n",
       "      <td>1.105026</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>-0.584460</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4  x5_a  x5_b  x5_c\n",
       "0    1.077412 -0.991094  0.208610 -1.019744     0     0     1\n",
       "1   -0.637747 -0.307506 -1.794250  1.189993     0     1     0\n",
       "2    1.345143 -1.075281  1.685044  0.167879     0     1     0\n",
       "3    1.659924  0.735156 -0.210541 -2.534105     0     1     0\n",
       "4    1.182378  1.599140 -0.301995  1.206648     0     1     0\n",
       "..        ...       ...       ...       ...   ...   ...   ...\n",
       "295  1.239739  1.572756  0.610548  1.402896     0     0     1\n",
       "296 -0.528292  3.120873  0.357688 -0.599724     0     0     1\n",
       "297  0.256334  1.293508  0.023925 -0.996928     0     1     0\n",
       "298 -0.628451  0.373513  0.038140  2.197234     0     1     0\n",
       "299  1.250264  1.105026  0.593161 -0.584460     0     1     0\n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfeatures_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the counts associated with the unique values of `x5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b    150\n",
       "c     75\n",
       "a     75\n",
       "Name: x5, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinput.x5.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 75 rows associated with the `x5='a'`. Next, look at the counts associated with the unique values of `x5_a` from the one-hot encoded feature array. Notice that there are just two values, 0 and 1. A value of 1 corresponds to `x5='a'`, while a value of 0 corresponds to `x5` **not** equaling `'a'`. Notice that below `x5_a=1` 75 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    225\n",
       "1     75\n",
       "Name: x5_a, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfeatures_onehot.x5_a.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the dummy variable approach, we tell `pd.get_dummies()` to drop the reference level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfeatures_dummy = pd.get_dummies(xinput, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how with the dummy variable approach there are just 2 additional *augmented* features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5_b</th>\n",
       "      <th>x5_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.077412</td>\n",
       "      <td>-0.991094</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>-1.019744</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.637747</td>\n",
       "      <td>-0.307506</td>\n",
       "      <td>-1.794250</td>\n",
       "      <td>1.189993</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.345143</td>\n",
       "      <td>-1.075281</td>\n",
       "      <td>1.685044</td>\n",
       "      <td>0.167879</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.659924</td>\n",
       "      <td>0.735156</td>\n",
       "      <td>-0.210541</td>\n",
       "      <td>-2.534105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.182378</td>\n",
       "      <td>1.599140</td>\n",
       "      <td>-0.301995</td>\n",
       "      <td>1.206648</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.239739</td>\n",
       "      <td>1.572756</td>\n",
       "      <td>0.610548</td>\n",
       "      <td>1.402896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.528292</td>\n",
       "      <td>3.120873</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>-0.599724</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.256334</td>\n",
       "      <td>1.293508</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>-0.996928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.628451</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>2.197234</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.250264</td>\n",
       "      <td>1.105026</td>\n",
       "      <td>0.593161</td>\n",
       "      <td>-0.584460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4  x5_b  x5_c\n",
       "0    1.077412 -0.991094  0.208610 -1.019744     0     1\n",
       "1   -0.637747 -0.307506 -1.794250  1.189993     1     0\n",
       "2    1.345143 -1.075281  1.685044  0.167879     1     0\n",
       "3    1.659924  0.735156 -0.210541 -2.534105     1     0\n",
       "4    1.182378  1.599140 -0.301995  1.206648     1     0\n",
       "..        ...       ...       ...       ...   ...   ...\n",
       "295  1.239739  1.572756  0.610548  1.402896     0     1\n",
       "296 -0.528292  3.120873  0.357688 -0.599724     0     1\n",
       "297  0.256334  1.293508  0.023925 -0.996928     1     0\n",
       "298 -0.628451  0.373513  0.038140  2.197234     1     0\n",
       "299  1.250264  1.105026  0.593161 -0.584460     1     0\n",
       "\n",
       "[300 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfeatures_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would use the dummy variable approach with sklearn, we would need to set the `fit_intercept` argument to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now confirm we can replicate the sklearn coefficient estimates when we use the one-hot and dummy augmented features from Pandas. Two models are fit below, the first uses the one-hot encoding features, while the second uses the dummy variable features. Notice how with the one-hot encoding setup, `fit_intercept` is `False`, while with the dummy variable approach `fit_intercept=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_fit_onehot = LogisticRegression(penalty='none', solver='lbfgs', fit_intercept=False).fit(xfeatures_onehot.to_numpy(), youtput.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_fit_dummy = LogisticRegression(penalty='none', solver='lbfgs', fit_intercept=True).fit(xfeatures_dummy.to_numpy(), youtput.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas creates one-hot and dummy *augmented* features are positioned at the end of the DataFrame. The one-hot and dummy *augmented* features created by `dmatrices()` are at the beginning of the design matrix. Thus, we need to reorder the coefficient estimates in order to properly compare them to what we estimated previously. The cell below manually defines the reordering for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_order = [4, 5, 0, 1, 2, 3]\n",
    "\n",
    "onehot_order = [4, 5, 6, 0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now perform the reordering using the `np.take()` function to take specific elements from a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_coefs = np.take(sk_fit_onehot.coef_.ravel(), onehot_order)\n",
    "\n",
    "dummy_coefs = np.take(sk_fit_dummy.coef_.ravel(), dummy_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the coefficients across all 4 models we fit. As shown below, the continuous feature coefficients are the same (within rounding). The fit from the statsmodels with the formula api (which includes the intercept column of 1s) is the same (within rounding) as the sklearn model that had the Pandas created input features using the dummy variable formulation. The sklearn fit with the one-hot encoded features from dmatrices (intercept column of 1s removed) and the Pandas created one-hot encoded features are also teh same (within rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statsmodels</th>\n",
       "      <th>sklearn_no_penalty_with_dummy</th>\n",
       "      <th>sklearn_no_penalty_from_dmatrices</th>\n",
       "      <th>sklearn_no_penalty_with_onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-0.674470</td>\n",
       "      <td>-0.674477</td>\n",
       "      <td>-0.674465</td>\n",
       "      <td>-0.674465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5[T.b]</th>\n",
       "      <td>-0.289844</td>\n",
       "      <td>-0.289849</td>\n",
       "      <td>-0.964304</td>\n",
       "      <td>-0.964304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5[T.c]</th>\n",
       "      <td>0.310042</td>\n",
       "      <td>0.310028</td>\n",
       "      <td>-0.364434</td>\n",
       "      <td>-0.364434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.310394</td>\n",
       "      <td>0.310397</td>\n",
       "      <td>0.310406</td>\n",
       "      <td>0.310406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.579591</td>\n",
       "      <td>-0.579592</td>\n",
       "      <td>-0.579577</td>\n",
       "      <td>-0.579577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.113856</td>\n",
       "      <td>0.113863</td>\n",
       "      <td>0.113846</td>\n",
       "      <td>0.113846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>-0.048754</td>\n",
       "      <td>-0.048754</td>\n",
       "      <td>-0.048760</td>\n",
       "      <td>-0.048760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           statsmodels  sklearn_no_penalty_with_dummy  \\\n",
       "Intercept    -0.674470                      -0.674477   \n",
       "x5[T.b]      -0.289844                      -0.289849   \n",
       "x5[T.c]       0.310042                       0.310028   \n",
       "x1            0.310394                       0.310397   \n",
       "x2           -0.579591                      -0.579592   \n",
       "x3            0.113856                       0.113863   \n",
       "x4           -0.048754                      -0.048754   \n",
       "\n",
       "           sklearn_no_penalty_from_dmatrices  sklearn_no_penalty_with_onehot  \n",
       "Intercept                          -0.674465                       -0.674465  \n",
       "x5[T.b]                            -0.964304                       -0.964304  \n",
       "x5[T.c]                            -0.364434                       -0.364434  \n",
       "x1                                  0.310406                        0.310406  \n",
       "x2                                 -0.579577                       -0.579577  \n",
       "x3                                  0.113846                        0.113846  \n",
       "x4                                 -0.048760                       -0.048760  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'statsmodels': stats_fit.params,\n",
    "              'sklearn_no_penalty_with_dummy': list(sk_fit_dummy.intercept_) + list(dummy_coefs),\n",
    "             'sklearn_no_penalty_from_dmatrices': list(sk_fit_mle.coef_.ravel()),\n",
    "             'sklearn_no_penalty_with_onehot': list(onehot_coefs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
